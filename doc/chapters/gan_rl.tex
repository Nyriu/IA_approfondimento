%!TEX TS-program = pdflatex
%!TEX root = main.tex
%!TEX encoding = UTF-8 Unicode


\section{GAN+RL per testi}
\todo[inline]{fare intro RNN? LSTM?}
In questa sezione vengono illustrati due modelli capaci di generare testi sintetici sfruttando un'architettura GAN in cui $G$ viene allenato attraverso \emph{reinforcement learning} .
Il primo modello, chiamato SeqGAN, è stato presentato in \cite{SeqGAN} ed illustrato anche in \cite{GAN_for_text}; il secondo è evoluzione del primo, permette di generare testi più lunghi, prende il nome di LeakGAN ed è descritto in \cite{LeakGAN}.
\todo[inline]{Sono particolarmente interessanti perché con SeqGAN si risolve} % TODO se serve

\subsection{SeqGAN}
Come riportato nell'introduzione dell'articolo \cite{SeqGAN}, per generare frasi che siano verosimili è necessario allenare un discriminatore che valuti frasi intere e che assegni a queste un punteggio.
Purtroppo ciò rende molto difficile allenare il generatore, perché non è possible determinare se un punteggio basso corrisponde all'intera struttura della frase oppure soltanto ad una o poche parole.
La problematica è ancora più evidente nel caso in cui il generatore è una RNN \todo{mai introdotte per ora, TODO da fare sopra}
rendendo difficile, ad esempio aggiornare efficacemente il modo con cui vengono create le parti iniziali di frasi.

Le SeqGAN affrontano il problema in un modo molto interessante: se si considera il punteggio che $D$ fornisce alle frasi come \emph{reward} per $G$ e se questo utilizza come stato la frase generata fino ad ora e come azione la scelta della parola successiva, allora è possibile sfruttare il \emph{Policy Gradient} sul generatore.
Di fondamentale importanza la \emph{Monte Carlo Search} che viene effettuata per valutare la bontà di frasi incomplete, così da alterare efficacemente la distribuzione della parola che ancora deve essere scelta: 
durante la generazione di una frase, $G$ non può ricevere una valutazione da $D$ perché il discriminatore è in grado di valutare soltanto frasi intere % TODO fare osservazione? % (una frase incompleta sarà sempre poco verosimile 
quindi vengono generate $N$ frasi con prefisso la frase generata fino ad ora.
Si sfrutta poi $D$ per valutare tutte le $N$ frasi e si effettua una media dei \emph{reward} ottenuti, così si ottiene il valore atteso della bontà della frase che si sta generando.
Ci si riferisce a questo furbo accorgimento come \emph{Monte Carlo state-action search}.

Riprendendo i formalismi usati nell'articolo si ha:
\begin{itemize}
  \item un modello generativo $G_\theta$, $\theta$ indica i parametri interni, in grado di generare sequenze $Y_{1:T} = ( y_1, \dots , y_t, \dots , y_T)$ con gli $y_t$ appartenenti all'insieme dei token validi $\T$;
  \item al tempo $t$ lo stato $s$ equivale ai token prodotti fino ad ora $(y_1, \dots , y_{t-1})$ mentre l'azione $a$ è il prossimo token da selezionare $y_t$;
  \item con $G_\theta (y_t | Y_{1 : t-1} )$ si indica il modello non deterministico descritto.

  \item Il modello discriminativo $D_\phi$ con parametri $\phi$ in grado di fornire la probabilità $D_\phi ( Y_{1:T})$ che $Y_{1:T}$ sia stato estratto dai dati reali.
\end{itemize}
% TODO \todo[inline]{qui immagine rete?}

Prima di continuare con la \emph{loss function} e la formulazione della \emph{Monte Carlo search}, 
va sottolineato che il modello RNN è leggermente diverso da quello classico, infatti ad ogni passo la rete prende in input il token generato al passo precedente anziché riceverlo dall'esterno.
Si può quindi dire che assomigli ai modelli RNN usati come decoder durante la traduzione di testi, nei quali lo stato interno e l'ultima parola tradotta vengono utilizzati per aggiornare lo stato e generare la parola successiva.
%Notare anche che la codifica interna 
Si fa presente che lo stato di partenza è TODO e l'input è parte da \todo[inline]{partenza con $s_0$ come viene fatta? $h_0$ com'è?}
Si può notare come questa sia la rivisitazione del tipico input randomico $z$ di una generica GAN.

L'obiettivo del generatore $ G_\theta $ è quello di produrre una sequenza a partire dallo stato $s_0$ che massimizzi il \emph{reward} totale, in formule:
$$
J(\theta) = \E[R_t | s_0, \theta ] =
\sum_{y_1 \in \T}
G_\theta ( y_1 | s_0) \cdot
Q_{D_\phi}^{G_\theta} ( s_0, y_1)
$$
in cui $Q_{D_\phi}^{G_\theta} ( s, a)$ è la funzione che indica il \emph{reward} accumulabile eseguendo l'azione $a$ allo stato $s$ e seguendo la \emph{policy} $G_\theta$ nei passi successivi.
Questa funzione dovrà necessariamente essere stimata, perché sappiamo che $D_\phi$ non può essere sfruttato su sequenze incomplete.
Quindi si utilizza una \emph{$N$-Monte Carlo Search} per stimare $N$ volte i $T-t$ token mancanti
$$
\{ 
  Y_{1:T}^1,
  \dots,
  Y_{1:T}^N
\}
=
MC (Y_{1:t}; N)
$$
Gli $ Y_{t+1:T}^n $ con cui si completa la sequenza di partenza sono campionati usando la stessa \emph{policy} $G_\theta$.
Quindi la stima del \emph{reward} atteso è data da
%$$
%Q_{D_\phi}^{G_\theta} ( s_0, y_1) TODO
%=
%\left\{
%\begin{array}{lr}
%  x(n), & \text{for } 0\leq n\leq 1\\
%\end{array}
%\right\
%$$

$$
Q_{D_\phi}^{G_\theta}
( s = Y_{1:t-1} ,
a = y_t)
=
\left\{\begin{array}{lr}
    \dfrac{1}{N} \sum_{n=1}^N D_\phi (Y_{1:T}^n) ,\; Y_{1:T}^n \in MC(Y_{1:t};N)
      & \textrm{for } t < T \\

    D_\phi (Y_{1:t}) & \textrm{for } t = T

\end{array}\right.
$$
%Abbiamo visto come $J(\theta)$ può essere calcolata e sappiamo che 

Per quanto riguarda il discriminatore $D_\phi$ viene specificato che l'aggiornamento dei suoi parametri $\phi$ viene effettuato solo quando il generatore ha creato un numero sufficiente di sequenze.
In questo modo è possibile avere un discriminatore che si adatta e migliora assieme al generatore, pur lasciandogli il tempo di perfezionarsi.
In formule $D_\phi$ viene allenato secondo:
$$
min_\phi
- \E_{Y \sim p_{real}} [ log D_\phi (Y) ]
- \E_{Y \sim G_\theta} [ log ( 1 - D_\phi(Y))]
$$
\todo{se tempo si può riportare schema algoritmo (appendice?)}
\todo[inline]{TODO spiegare cos'è il pre-train e come viene fatto}


